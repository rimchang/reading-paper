# Object Detection in Videos with Tubelet Proposal Networks

### Abstract

대규모의 ImageNet VID 데이터셋이 소개됨에 따라 비디오에서의 object detection이 주목을 받고 있습니다. static image에서의 object detection과 달리 비디오에서의 temporal information은 매우 중요합니다. temporal information을 최대한 활용하기 위해 최신의 [15, 14] 는 spatio-temporal tubelet을 기반으로 합니다. tubelet은 시간에 따라 연속되는 sequence of bounding box 입니다. 그러나 기존의 방법은 tubelet을 생성하는데 있어 성능과 효율의 한계가 있습니다. Motion-based [14] 방법은 효율적으로 dense tubelet을 얻을 수 있지만 짧은 프레임의 tubelet밖에 생성을 못하고 long-term temporal information을 모델링 할 수 없습니다. Appearance-based [15] 방법은 일반적인 object tracking 방법을 포함하며 긴 tubelet을 생성 할 수 있지만 계산량이 많습니다. 우리의 논문에서는 비디오에서의 object detection을 위한 프레임워크를 제안하고 이는 효율적으로 spatio-temporal proposal을 생성할 수 있는 tubelet proposal 네트워크입니다. 그 이후 LSTM을 활용하여 tubelet proposal에서의 temporal information을 통합합니다. 매우큰 데이터인 VID 에서의 실험을 통해 제안한 프레임워크의 효율을 보여줍니다.

### 1. Introduction

object detection의 성능은 딥 네트워크의 등장으로 크게 향상되었습니다. GoogLeNet[29], VGG[27], ResNet[8] 과 같은 새로운 뉴럴네트워크 구조는 ImageNet 데이터셋에서의 학습 능력을 향상시키기 위해 제안되었고 object detection, semantic segmentation, scene understanding, person serach, 등에 적용되었습니다. static 이미지에 대한 최근의 object detection 프레임워크는 이러한 CNN구조를 기반으로 하며 3가지의 주요 단계로 구성되었습니다. 이미지로 부터 RPN 혹은 외부 모듈을 통해 bounding box proposal을 먼저 생성하고 각 proposal이 object을 포함할지 안할지를 예측합니다. 각 box proposal 로 부터 apperance feature를 추출하여 object class중 하나로 이들을 판별합니다. 이러한 bounding box와 관련된 class socre는 nms와 같은 post-processing 기술을 통해 refine 되고 최종 detection 결과를 얻게 됩니다. Fast R-CNN과 Fast R-CNN과 같은 다양한 프레임워크는 이러한 연구 방향을 따르고 object detection 문제를 end-to-end 학습을 가능하게 만들었습니다.

static 이미지에서의 object detection은 큰 성공을 거두었지만 비디오에서의 object detection은 여전히 어려운 문제로 남아 있습니다. 비디오에서의 object detection을 어렵게 하는 몇몇 요소가 있습니다. 시간에 따라 같은 object의 급격한 apperance, sacle의 변화가 생기며 occlusion, motion blur, 이미지 데이터와 비디오 데이터간의 불일치 등이 주요 요소입니다. 2015년도에 비디오에서의 object detection 벤치마크인 VID 챌린지가 소개되었고 이는 각 30개의 카테고리르 가지며 각 프레임마다 라벨링이 되어져 있습니다. VID에 영향을 받아 [7, 14, 15]와 같은 모델들이 static image object detector를 비디오로 확장하기 위해 제안되어졌습니다.

static object detection 에서의 bounding box proposal과 비슷하게 비디오에서 bounding box에 대응되는 것은 tubelet이라 불리며 bounding boxes proposal의 sequence 입니다. video object detection을 위한 최신의 알고리즘은 tublet을 이용하여 detection result를 얻기 위한 temporal information 얻도록 모델을 확장합니다. 그러나 tublet을 생성하는 것은 frame-by-frame 의 detection 결과를 기반으로 하므로 매우 많은 시간이 들게 됩니다. 예를들어 [14, 15] 에서 사용된 tracking 알고리즘은 각 프레임에서의 하나의 detection box를 처리하는데 0.5 초의 시간이 들게 되며 이는 허락된 시간내에 시스템에 질 좋은 tublet proposal을 생성하지 못하게 합니다. 비디오는 수백개의 프레임을 포함하고 각 프레임마다 수백개의 detection box를 포함하기 때문에 상상 이상의 시간이 소모됩니다. optical-flow-guided propagation [14] 와 같은 motion-based 모델들은 dense tublet을 효율적으로 생성하지만 long-term tarcking의 일관성 없는 성능 때문에 보통 매우 짧은 프레임([14] 에서는 7 length) 로 제한 됩니다. 비디오에서의 object detection을 위한 이상적인 tublet은 충분히 길어 temporal information을 잘 통합 할 수 있어야 하며 다양한 길이를 생성해 recal rate를 높일 수 있어야 합니다.

이러한 문제를 완화하기 위해서 비디오에서의 object detection을 위한 새로운 프레임워크를 제안합니다. 이는 Tubelet Proposal Network(TPN) 과 LSTM sub-net으로 구성됩니다. TPN은 static proposal 로 부터 수백개의 다양한 tubelet을 동시에 생성하고 LSTM은 tubelet의 temporal information을 기반으로 object confidence를 추정합니다. TPN은 feature map pooling을 통해 효율적으로 tubelet proposal을 생성합니다. start 프레임에서 static box proposal 이 주어지면 우리는 TPN과 같은 multi-frame regression 뉴럴 네트워크를 학습하기 위해 프레임에 걸쳐 같은 box location을 pooling 합니다. 이는 foreground의 복잡한 motion 패턴을 학습할 수 있어 로버스트한 tubelet proposal을 만들 수 있또록 합니다. 비디오에서의 수백개의 proposal은 동시에 tracking 할 수 있습니다. 이러한 tubelet proposal은 각 프레임에서 개별적으로 얻은 결과보다 더 나은 품질을 보이며 이는 비디오에서의 temporal information의 중요성을 나타냅니다. tubelet box로 부터 추출된 feature들은 자동적으로 feature sequence로 정렬되며 LSTM과 같은 네트워크를 통해 temporal feature를 학습하기에 적절합니다. LSTM은 정확한 proposal classfication을 위한 long-term temporal dependency를 포착하기에 적합합니다.

이 논문의 공헌은 tubelet proposal 생성과 temporal classfication을 결합하는 새로운 프레임워크를 제안하는 것입니다. 효율적인 tubelet proposal 생성 알고리즘은 비디오에서의 object의 spatio-temporal information을 포착하기 위해 만들어 졌습니다. temporal LSTM 모델은 visual feature, temporal feature를 모두 가진 tubelet proposal을 판별하기 위해 채택되었습니다. 이러한 high-level temporal feature는 기존의 detection 시스템에서는 무시되었지만 비디오에서의 object detection에서는 매우 중요합니다.

### 2. Related work

<b>Object detection in static images.</b> 최신의 object detection 시스템은 CNN을 기반으로 합니다 [6] 에서 Girshick은 RPN, CNN finetuning, region classfication의 여러 단계로 object detection 문제를 분해하는 R-CNN 을 제안했습니다. R-CNN의 학습 과정을 빠르게 하기 위해 Fast R-CNN이 제안되었고 모든 bouding box proposal 에 대해 CNN 연산을 하는 것을 피하는 방법을 제안했습니다. 같은 이미지의 여러개의 bounding box의 feature는 ROI pooling 연산을 통해 효율적으로 계산됩니다. bounding box proposal의 성능을 높이기 위해서 Faster R-CNN은 RPN 이라는 것을 도입하였고 뉴럴 네트워크를 통해 직접적으로 box proposal을 생성 가능합니다.

<b>Object detection in videos.</b> ImageNet 챌린지에서 VID task가 도입된 이래로 비디오에서의 object detection을 위한 여러가지 시스템이 있었습니다. 이러한 방법들은 static-image detector score의 temporal consistency를 높이기 위해 post-processing class score에 초점을 맞췄습니다. [7] 에서 Han은 관련된 초기의 detection 결과를 sequence로 변환합니다. 비디오에서의 sequence에 따른 weaker class score는 frame-by-frame detection 결과를 향상시키기 위해 개발되었습니다 [15] 에서 Kang은 static-image bounding box proposal에 tracking 알고리즘을 적용하여 새로운 tubelet proposal을 생성했습니다. 먼저 static image detector에 의해 구해진 class score는 1D CNN에 의해 rescoring 됩니다. 같은 그룹의 [14] 는 tublet classfication, re-scoring을 위한 다른 전략을 취합니다. 초기의 detection box는 optical flow에 의해 근처의 프레임으로 전파되며 max class score가 아닌 tublet을 suppress하여 class score의 temporal consistency를 향상 시켰습니다.

<b>Object localization in videos.</b> 비디오에서의 object localization에 관한 연구와 데이터셋들이 있습니다. 그러나 이들의 문제 설정은 너무 단순하여 각 비디오가 하나의 객체만을 포함하는 것으로 가정되며 각 프레임마다 오직 하나의 객체만을 예측합니다.

### 3. Tubelet proposal network

비디오에서의 object detection을 위한 기종의 방법들은 일반적인 key 프레임에서 시작하는 single-object tracker 혹은 tracking-by-detection 과 같은 data association 방법을 사용해 tubelet proposal을 생성합니다. 이러한 방법은 충분한 dense tublet을 얻기에 연산량이 너무 비싸거나 tracking이 실패 할 수 가 있습니다. 100 fps의 single-object tracker가 있더라도 프레임당 300개의 bounding boxes를 생성하는 것에는 약 56 GPU day의 시간이 걸릴 수 있습니다.

<img src="data:image/png;base64,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" />

우리는 TPN이라는 것을 제안하고 비디오에서의 tubelet proposal 생성을 효율적으로 할 수 있습니다. 피규어 2에 보는 바와 같이 TPN은 주요한 2가지 요소로 구성됩니다. 첫번째 sub-network는 single frame 에서의 static region proposal 결과를 기반으로 visual feature를 추출합니다. 우리의 주요한 관찰은 CNN의 receptive field는 일반적으로 충분히 커서 매우 단순하게 같은 bounding box location으로 time 축으로 max pooling을 하더라도 움직이는 객체의 visual feature를 추출 할 수 있습니다. 두번재 구성요소는 pooled visual feature를 기반으로 bounding box의 temporal displacement를 추정하는 regression layer 입니다.

### 3.1. Preliminaries on ROI-pooling for regression

object detection을 위해 feature map pooling을 이용하는 기존의 방법들이 존재합니다 Fast R-CNN은 visual feature에 ROI-pooling을 사용하고 object classfication, bounding box regression을 수행합니다. CNN은 인풋 이미지를 받아 forward 연산을 수행해 visual feature map을 생성합니다. 몇몇의 object proposal이 주어진다면 propsoal에 해당하는 visual feature는 box coordinate에 해당하는 ROI-pool 을 통해 직접적으로 구해질 수 있습니다. 이러한 방법을 통해 CNN은 하나의 인풋 이미지에 대해 한번의 forward propagate만을 연산하면 되어 연산량을 매우 많이 절약 할 수 있습니다. b_t^i = (x_t^i,y_t^i, w_t^i, h_t^i) 라고 정의하면 b_t^i는 t번째 time의 staitc box proposal이 되며 x,y,h,w 는 box proposal의 center, height, width에 해당하게 됩니다. ROI-pooling을 통해 b_t^i 에 해당하는 visual feature r_t^i 를 얻을 수 있습니다.  

각각의 bounding box proposal에 해당하는 ROI-pooled feature r_t^i 는 object classfication에 사용될 수 있고 흥미롭게도 bounding box regression에도 동시에 사용될 수 있습니다. 이는 ROI-pooling을 통해 얻어진 visual feature가 object의 위치를 설명하는데 필요한 정보를 가지고 있다는 것을 나타냅니다. 이러한 기술에 영감을 받아 우리는 ROI-pooling을 통해 multiframe visual feature를 추출하는 것을 제안하고 이러한 feature를 regression을 통해 tublet proposal을 생성합니다. 

### 3.2. Static object proposals as spatial anchors

static object proposal은 object가 있을 만한 위치를 나타내는 클래스와 상관없는 bounding box를 생성합니다. 이는 SelectivSearch, EdgeBoxes, RPN과 같은 다양한 방법들에 의해 효율적으로 얻어질 수 있습니다. 그러나 비디오에서의 object detection 에서는 object의 sptail, temporal location을 모두 알아야 합니다. 정확한 proposal classfication을 위해서는 temporal information을 이용하는 것이 매우 중요합니다.

일반적인 비디오에서의 객체의 움직임은 매우 복잡하고 예측하기 어렵습니다. static obejct proposal은 개별 프레임에 대해서는 매우 높은 recall 을 보입니다(90% 이상) 이는 매우 중요한데 obejct detection의 성능의 상한선이 됩니다. 그러므로 static proposal을 일종의 starting anchor로 사용하여 proposal tubelet에서의 object의 움직임을 추정하는 것이 자연스럽습니다. 만약 object의 움직임을 로버스트하게 추정할 수 잇다면 다음 time의 높은 recall rate를 기대할 수 있습니다.

time t=1 의 static proposal 을 b_1^i 라고 합시다. b_1^i 에서 시작하는 tublet proposal을 생성하기 위해 w-프레임의 window를 통해 visual feature가 추출되며 1~w 의 프레임 동안 같은 location b_1^i의 위치에서 pooling이 수행되게 됩니다. 이렇게 추출된 visual feature를  r_1^i,,,r_w^i 라고 합시다. 우리는 이러한 b_1^i 를 일종의 spatail anchor라고 부릅니다. pooled regression feature는 객체의 visual appearance를 부호화 하며 visual feature(r_1^i,,,r_w^i)의 대응 관계를 복구 함으로써 정확한 tublet proposal을 얻을 수 있습니다. 이는 regression layer로 모델링 되며 자세한 것은 다음 섹션에 기술됩니다.

같은 spatial location의 위치에서 multi-frame feature를 pooling 할 수 있는 이뉴는 higher layer의 CNN feature가 매우 큰 receptive fields를 가지기 때문입니다. 만약 매우 작은 bounding box로 부터 pooling 된 visual feature가 있다고 해도 visual context는 boudning box의 크기보다 크게 됩니다. 같은 box location에 pooling을 하는 것은 object의 다양한 움직임을 포착 할 수 있게 됩니다. 피규어 2에는 tublet proposal 생성을 위한 spatial anchor를 보여줍니다. 같은 loaction의 feature는 시간순으로 잘 정렬되어 있어 object의 움직임을 예측합니다.

우리는 BN을 사용한 GoogleLeNet 모델을 TPN을 위해 사용합니다. 우리의 세팅에서는 ROI-pooling layer는 "inception 4d" 에 연결되며 이는 363 pixel의 receptive field를 가집니다. 그러므로 ROI-pooling을 같은 box location에 수행해도 363 pixel의 움직임을 포착 가능합니다. 이는 short-term object 움직임을 포착하기에 충분합니다. 각각의 static proposal은 temporal window W 안에서 feature extarction을 위한 achor point로 사용됩니다.

### 3.3. Supervisions for tubelet proposal generation

우리의 목표는 high recall rate를 가지고 정확하게 object를 tracking 할 수 잇는 tubelet proposal을 생성하는 것입니다. box loaction b_t^i 에서 pooling 된 visual feature를 사용하여 우리는 regression 네트워크 R을 학습시키고 이는 spatial anchor 에 관련된 relative movement를 효율적으로 추정합니다.

 <img src="http://latex.codecogs.com/gif.latex?m_1%5Ei%2C%20m_2%5Ei%2C%20%5Ccdots%20%2C%20m_w%5Ei%20%3D%20R%28r_1%5Ei%2C%20r_2%5Ei%2C%20%5Ccdots%20%2C%20r_w%5Ei%29%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%281%29"/>

relative movement m_w^i는 다음과 같이 계산됩니다. 


 <img src="http://latex.codecogs.com/gif.latex?%5CDelta%20x_t%5Ei%20%3D%20%28x_t%5Ei%20-%20x_1%5Ei%29/w_1%5Ei%20%2C%20%5C%2C%5C%2C%5C%2C%5C%2C%20%5CDelta%20y_t%5Ei%20%3D%20%28y_t%5Ei%20-%20y_1%5Ei%29/h_1%5Ei%20%5C%5C%5B12pt%5D%20%5CDelta%20w_t%5Ei%20%3D%20log%28w_t%5Ei/w_1%5Ei%29%20%2C%20%5C%2C%5C%2C%5C%2C%5C%2C%20%5CDelta%20h_t%5Ei%20%3D%20log%28h_t%5Ei/h_1%5Ei%29"/>

(temporal window W\*4 만큼의 regression을 수행하네.. 첫번째 프레임의 initial proposal box를 가지고 W길이 만큼의 temporal regression을 수행한다.) 이러한 relative movement를 얻고 나면 실제의 box location을 쉽게 추론 가능합니다. 우리는 visual feature (r1, r2, r3,,, rw)^T 를 concat 하고 이를 인풋으로 받는 fc-layer를 사용하여 W\*4 만큼의 movement 값을 예측합니다. 

 <img src="http://latex.codecogs.com/gif.latex?%5Bm_1%5Ei%2C%20%5Ccdots%20%2C%20m_w%5Ei%5D%5ET%20%3D%20W_w%5Br_1%5Ei%2C%20%5Ccdots%20%2C%20r_w%5Ei%5D%5ET%20&plus;%20b_w"/>

fwx4w 의 차원인 W_w와 4w 차원인 b_w 는 fc-layer에서 학습 가능한 파라미터 입니다.

이제 남은 문제는 relative movement(init anchor로 부터의 상대적인 box의 움직임) 에 대한 적절한 supervision을 제공하는 일입니다. 우리의 주요한 가정은 tublet proposal이 ground-truth object 와의 consistent movement 패턴을 가져야 한다는 것입니다. 그러나 static object proposal이 tubelet 생성의 start box로 주어지면 이들은 일반적으로 ground truth object box와 100% IOU를 가질 수가 없습니다. (start box가 틀려버리면 뒤의 tubelet도 다 틀려버린다는 의미인듯?) 따라서 우리는 ground truth box와의 movement 패턴을 따르기 위해서는 static box proposal이 ground truth box와 가깝도록 해야 합니다. 더 구체적으로는 만약 static object proposal b_t^i 가 어떤 grount truth box b^_t^i 와의 IOU가 0.5 이상이고 이것이 가장 많이 겹치는 ground truth box 라면 우리의 regression layer는 가장 IOU가 큰 b^_t^i 와 같은 movement 패턴을 갖도록 해야 합니다. (IOU 가 0.5 이상인 여러개의 ground-truth가 있따면 가장 큰 IOU를 갖는 gt와 모션이 비슷하도록 해야 한다.) relative movement target m^_i^t 은 ground truth box b^_t^1 에 의해서 정의될 수 있습니다. 이는 eq(2) 와 비슷한 방법으로 정의됩니다. 만약에 m^_1^i= (0,0,0,0) 이라면 우리는 2번째 부터 w번째 가지에 대해 예측하면 됩니다. 첫번째 프레임에서의 spatial anchor에 대한 relative movement를 배우는 것은 tracking 알고리즘으로 인한 누적되는 오류를 피할 수 있습니다.

movement target은 그들의 평균, 분산을 통해 normalization 되어 집니다.

 <img src="http://latex.codecogs.com/gif.latex?%5Ctilde%7Bm_t%5Ei%7D%20%3D%20%28%5Chat%7Bm_t%5Ei%7D%20-%20%5Cbar%7Bm_t%7D%29/%5Csigma_t%2C%20%5C%2C%5C%2C%20for%20%5C%2C%5C%2Ct%20%3D%201%2C%5Ccdots%20%2Cw%20%5C%2C%5C%2C%5C%2C%284%29"/>

관련된 ground truth box의 movement 패턴을 따르는 N개의 tubelet을 생성하기 위해 우리는 다음과 같은 objective function을 최소화 합니다.

 <img src="http://latex.codecogs.com/gif.latex?L%28%7B%5Ctilde%7BM%7D%2C%20M%29%20%3D%20%5Cfrac%7B1%7D%7BN%7D%20%5Csum_%7Bi%3D1%7D%5EN%5Csum_%7Bt%3D1%7D%5Ew%7D%20%5Csum_%7Bk%5Cin%20%7Bx%2Cy%2Cw%2Ch%7D%7D%20d%28%5CDelta%20k_t%5Ei%29%20%5C%2C%5C%2C%5C%2C%5C%2C%20%285%29%22"/>



 {tilde_M} 과 {M} 은 모두 normalized movement target이며 네트워크의 아웃풋은 다음과 같습니다.
 
  <img src="http://latex.codecogs.com/gif.latex?d%28x%29%20%3D%20%5Cbegin%7BBmatrix%7D%200.5x%5E2%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%20if%20%5C%2C%20%7Cx%7C%20%3C%201%20%5C%5C%20%7Cx%7C-0.5%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%20otherwise%20%5Cend%7BBmatrix%7D"/>
 

 
 이는 [5] 에서 사용되는 smoothed L1 loss 입니다. 네트워크의 최종 아웃풋은 실제 realative movement로 다시 바뀌어 집니다.
 
  <img src="http://latex.codecogs.com/gif.latex?m_t%5Ei%20%3D%20%28%5Cdot%7Bm_t%5Ei%7D%20&plus;%20%5Cbar%7Bm_t%7D%29%20*%20%5Csigma_t%20%5C%2C%5C%2C%5C%2C%5C%2C%5C%2C%20%287%29"/>
 
 
 우리의 정의에 따라 만약 static object proposal이 몇몇 object를 포함하고 있다면 이후 레이어에도 같은 정도의 비율로 object를 포함하고 있어야 합니다.

### 3.4. Initialization for multi-frame regression layer

TPN에서의 temporal window W도 매우 중요한 요소입니다. 가장 간단한 모델은 2 프레임 모델입니다. 주어진 프레임에 대해 현재 프레임의 spatial anchor의 feature가 다음 프레임(2 프레임 모델이기 때문에) 의 feature와 concat 되고 다음 프레임의 movement b_1^i 를 추정합니다. 그러나 2-frame 모델은 매우 짧은 temporal window를 사용하기 때문에 생성된 tubelet은 부드럽지 않고 꺾이는 경우가 많습니다. temporal window 를 늘리는 것은 더 많은 temporal information을 사용하게 하고 더 복잡한 movement 패턴을 추정할 수 있게 합니다.

temporal window size W 가 주어지면 이에서 추출되는 feature의 차원은 fw이며 여기서 f는 single frame에서의 feature의 채널입니다. (우리의 세팅에서는 inception_5b에서 추출된 1024 채널의 feature 입니다.) 그러므로 regression layer의 파라미터 사이즈는 fwx4w로써 temporal window size w에 쿼드라틱 하게 늘어나게 됩니다.

temporal window가 큰 경우 파라미터를 랜덤하게 초기화 하는 것은 regression layer를 학습하는데 어려움이 있습니다. 우리는 'block' initialization 방법을 제안하여 2-프레임 모델로 부터 학습된 feature를 사용하여 multi-frame model을 초기화 합니다.

피규어 3에서는 5-프레임 모델을 초기화 하기 위해 어떻게 pre-trained 2-프레임 모델을 사용하는지를 보여줍니다. 수식(2) 의 goal은 항상 (0,0,0,0) (gt box와의 차이가 0이 되는 것이 optimal) 이기 때문에 나중의 프레임의 movement를 추정할 필요가 있습니다. (뭔소리지..?) 2-프레임 모델의 regression layer의 파라미터 사이즈는 2fx4 가 되게 되고 bias term은 4의 차원이 됩니다. 5-프레임 모델의 경우에는 5fx(4x4) 의 weight와 (4x4) 차원의 bias term이 존재합니다. (4 box coordinate x (5-1) 의 차원이다. 맨 처음의 anchor 에 대해서는 이미 box가 주어져 있음.) 본질적으로 우리는 프레임 1,2 의 visual feature를 사용하여 프레임 2의 movement를 추정하고 프레임 1,3 feature를 사용해 프레임3의 movement를 추정합니다. (spatial anchor랑.. i번째 프레임의 feature를 사용하여 relative movement를 구하는 것임으로 근본적으로 그렇다는 것인듯?) 그러므로 5-프레임 모델의 weight는 두개의 sub-matrix로 나눠질 수 있습니다. 그림과 같이 초기화를 하며 bias term은 2-프레임 모델의 bias를 4번 반복합니다. (그림에서의 하얀부분에 대해서 생각을 해보자면 첫번째 열에 대한 해석은 두번째 프레임의 relative movement를 구하기 위해서 필요한 1번째 프레임에 대한 가중치, 2번째 프레임에 대한 가중치, 3번째 프레임에 대한 가중치,,,,5번째 프레임에 대한 가중치로 해석 할 수 있을 것 같다. 그림 처럼 초기화를 한다는 것은 2번째 프레임의 movement를 구하기 위한 2번째 프레임에 곱해지는 가중치를 2-프레임 모델의 파라미터로 초기화 하겠다.)

우리의 실험에서는 먼저 2-프레임 모델을 랜덤 하게 초기화 하여 학습시키고 2-프레임 모델을 multi-프레임 regression layer의 초기값으로 사용합니다.

### 4. Overall detection framework with tubelet generation and tubelet classification

TPN 을 기반으로 하여 우리는 비디오에서의 object detection을 위한 효율적인 프레임워크를 제안합니다. sigle object tracker와 비교하여 이는 VID 데이터셋에서 dense tubelet을 생성하는데 TPN 9 GPU day가 소요되며 이는 detection 성능을 높이기 위해 tubelet proposal 로 부터 temporal information을 활용 할 수 있습니다. 피규어 2에서 볼 수 잇는 바와 같이 우리의 프레임워크는 2개의 네트워크로 구성됩니다. 첫번째는 candidate object tubelet을 위한 TPN 이며 두번째는 CNN-LSTM classfication 네트워크이며 이는 tublet에 속하는 각각의 bounding box의 object 라벨을 예측합니다. (bouding box 마다의 클래스피케이션을 하는게.. 말이되나? 나중에 average를 하나?)

### 4.1. Efficient tubelet proposal generation 

TPN은 temporal window W 내의 static object proposal에 대한 movement를 추정 할 수 잇습니다. 대규모 비디오 데이터 셋에서의 object dtection을 위해선 몇백개의 spatial anchor에 대한 tubelet을 병렬적으로 만들고 각각의 tubelet이 충분한 길이를 가지도록 해야 합니다.

피규어 4 에서 볼 수 있듯이 l의 길이의 tubelet을 생성하기 위해서는 첫번재 프레임의 static object proposal을 spatial anchor로 사용하고 w temporal window를 가지는 TPN을 길이가 l이 될때 까지 반복해야 합니다. 이전 반복의 마지막 location의 추정을 다음 반복의 spatial anchor로 사용됩니다. 이 과정을 통해 임의의 길이를 가지는 tubelet proposal을 생성할 수 있습니다.

N static object proposal이 start frame에 존재한다면 CNN은 이들에 대해 foward 연신을 단 한번만을 수행하면 됩니다. 그래서 몇백개의 tubelet proposal을 효율적으로 생성 가능합니다. (spatial anchor를 구하고.. anchor를 변경하지 않는다. 그렇다면 영상 중간에 새로 나타나는 객체는 어떻게.. 처리하지??)

일반적으로 적용되는 이전의 single object tracker와 비교하여 우리의 제안된 방법은 tubelet을 생성하는데 훨씬 더 빠릅니다. [15] 의 tracking 방법은 단일 객체에 대해 0.5 fps 의 속도를 보고 했습니다. 보통 프레임당 300개의 spatial anchor가 존재하므로 이는 각 프레임당 150sec 가 걸리게 됩니다. 우리의 방법은 프레임당 0.488 sec 가 걸리며 이는 약 300배 빠릅니다. 최근의 single object tracker [9] 와 비교해도 우리의 방법은 6.14 배 빠른 속도를 보입니다.

### 4.2. Encoder-decoder LSTM (ED-LSTM) for temporal classification

길이가 l 인 tubelet proposal을 생성한후 visual feature u_t^1 ~ u_l^i 의 feature를 각각 tubelet의 coordinate를 이용하여 pooling 할 수 있습니다. [15, 7, 14] 와 같은 기존 방법들은 post-processing에 주로 temporal information을 사용하여 detecion 결과를 인접 프레임에 전파하거나 detection score를 temporally smoothing 하였습니다. detection 결과의 temporal consistency는 매우 중요하지만 복잡한 appearance의 변화를 포착하기 위해 tubelet을 사용해야 합니다. 우리는 각각의 tubelet location에서 discriminative spatio-temporal feature(classfication을 위해 사용될 feature?)를 배울 필요가 있습니다. 

피규어 2에서 볼 수 있듯이 제안된 classfication sub-network는 CNN이 포함되어 있어 인풋 이미지에 대한 feature를 추출할 수 있습니다. 각각의 tubelet proposal 에서의 각 time에 해당하는 ROI-pooled classfication feature는 그후 1-레이어의 LSTM의 인풋으로 들어가고 tubelet classfication을 수행하게 됩니다. LSTM은 RNN의 특별한 종류로써 최근들어 spatio-temporal feature를 학습하게 위해 많이 사용되엇습니다. 각각의 LSTM 유닛은 메모리 유닛을 가지고 있어 각 time 마다의 visual information을 기억해 temporal information을 배울 수 있습니다. 

LSTM의 각 time step t 마다의 인풋은 cell state, hidden state , 그리고 현재 time t에 해당하는 classfication feature u_t^i 를 인풋으로 받습니다. LSTM의 start state는 제로로 초기화 되며 각 time 마다의 hidden stat h_t^i 는 fc-layer의 인풋으로 들어가 class confidence를 예측하고 다른 fc-layer를 통해 regression을 수행합니다. vanilla LSTM을 사용함으로써 겪을 수 있는 문제는 initial state 가 처음 몇몇의 프레임의 classfication 결과에 큰 영향을 끼친다는 것입니다. [28]의 seq-to-seq LSTM에 영감을 받아 우리는 encoder-decoder LSTM 구조를 사용합니다. 첫번째 tubelet의 feature는 encoder  LSTM의 인풋으로 들어가고 unrolling 되면서 전체 tublet의 appearance feature를 메모리로 저장하게 됩니다. 그 후 각 t번째 tubelet에 해당하는 feature와 hidden state가 decoder LSTM의 인풋으로 들어가게 되고 last 프레임부터 첫번째 프레임 까지의 reverse input order로 classfication을 수행하게 됩니다. (각 프레임별 term 이 동일하도록 하기 위해!!) 이러한 방법을 통해 과거와 미래의 information을 모두 사용하여 더 좋은 classfication 성능을 달성 할 수 있습니다. all-zero init memory state에 의한 낮은 prediction confidence를 피할 수 있습니다.

### 5. Experiments
### 5.1. Datasets and evaluation metrics

제안된 프레임워크는 이미지넷의 VID 데이터 셋에서 평가됩니다. 30개의 object 클래스가 존재하며 데이터셋은 세개의 서브셋으로 나눠지며 트레이닝 셋에는 3862개의 비디오, validation 셋에는 55개의 비디오, 테스트 셋에는 937 개의 비디오를 포함합니다. 모든 비디오 프레임 마다의 ground truth가 라벨링 되어 있으며 테스트 셋에 대한 라벨링은 공개되어 있지 않으므로 일반적인 관행인 validation 셋에 대한 결과를 보고합니다. 30개의 클래스에 대한 map가 evalution metric으로 사용됩니다.

또한 우리의 시스템은 YouTubeObject 데이터셋에 대해 평가하고 이는 object localization task에 대한 데이터입니다. YTO 데이터셋은 10개의 클래스로 이루어져 있으며 이 10개는 VID 데이터셋의 sub-label set 입니다. YTO 데이터셋은 비디오에 대해 단 하나의 ground-truth 만이 라벨링 되어 있어 이를 오직 evaluation으로만 사용합니다. evaluatation metric은 CorLoc 를 사용하고 이는 ground truth box와 0.5 이상의 IOU를 갖는 recall rate를 말합니다.

### 5.2. Base CNN model training

우리는 BN을 사용한 GoogleNet을 우리의 베이스 CNN 모델로 선택하였고 우리의 TPN과 CNN-LSTM 모델이 weight를 공유하지 않게 하였습니다. 이 모델은 는 ImageNet classfication으로 부터 pre-train 되었고 VID 데이터 셋에 대해 fine-tuning 을 수행했습니다. static object proposal 은 VID 데이터로 부터 학습된 RPN으로 부터 생성되며 per-frame당 recall rate는 각 프레임당 300개의 box에 대해 95.92% 를 보입니다.

Fast RCNN 프레임워크와 결합하기 위해 우리는 마지막 모듈 'inception 5d'가 아닌 'inception 4d' 뒤에 ROI-pooling을 수행하였습니다. 이는 마지막 모듈은 32 down-sampling에 해당되어 receptive field가 715 pix가 됩니다. 이는 discriminative feature를 위해서는 너무 큰 receptive field 이기 때문입니다. ROI-pooling 의 사이즈는 14x14 이며 우리는 이후 마지막 inception module을 적용하고 마지막에 global average pooling을 적용합니다. 이 후 tubelet proposal, classfication, bounding box regression을 위한 fc-layer를 추가합니다.

4개의 titan x gpu를 사용해 200k iteration을 학습하였고 각 iteration 마다 하나의 gpu에 2개의 이미지의 32 ROI 를 통해 학습합니다. (ROI 사이즈가 너무 작은거 아닌가? proposal region은 300개고.. 마지막 batch가 32개라는 건가?) 초기 learning rate는 5x10^-4 이며 60k 마다 10씩 감소합니다. 모든 BN layer는 fine-tune 동안 고정합니다. DET 데이터에 대해 fine-tuning 후에 DET에 대해선 50.3% map를 달성했습니다. 같은 하이퍼 파라미터를 사용하여 VID 데이터에 대해 90.000 iteration 동안 학습하였고 VID validation set에 대해 63.0% map를 달성했습니다.

### 5.3. TPN training and evaluation

fine-tuning된 모델을 사용하여 VID 데이터에 대해 2-프레임 모델을 먼저 학습시킵니다. TPN이 gt object의 움직임에 따른 tubelet을 추정해야 하기 때문에 우리는 static proposal이 IOU가 0.5 보다 높은 anchor를 spatial anchor로 선택합니다. IOU가 0.5 보다 낮은 proposal들은 학습 동안에 사용되지 않습니다. 테스트 타임에는 매번 20 프레임마다의 모든 static object가 spatial anchor로 사용됩니다. 모든 tubelet은 20-프레임의 길이를 가집니다. negative static proposal 에서 시작한 것들은 background region 에 계속 남아 있게 되거나 근처의 프레임에서 object가 나타나면 그들을 track 하게 됩니다.

우리는 다양한 window size w에 대해 실험을 진행하였고 섹션4에 기술된 초기화 메소드를 사용했습니다. gt movement 을 gt label을 통해 구할 수 있으므로 각각의 positive static proposal은 그와 관련된 gt movement 와의 이상적인 tubelet proposal을 가집니다. 생성된 tubelet에 대한 세가지의 메트릭을 사용하였고 이는 테이블 1에 나와있습니다. 하나는 mean absolute pixel difference(MAD) 이며 이는 예측된 좌표와 ground truth 사이의 픽셀 값을 비교합니다. 두번째는 mean relative pixel differnece(MRD) 이며 gt와의 x좌표 차이/width, y좌표차이/height를 보고합니다. 세번재는 mean itersection-over-union (IOU) 를 보고합니다.

테이블을 살펴보면 2-프레임 베이스라인의 MAD 15.50 MRD 0.07 IOU 0.79 를 볼수 있으며 5-프레임 모델의 경우 랜덤하게 초기화를 하는 경우 2-프레임 모델에 비해 성능이 떨어집니다. 이는 5-프레임 모델의 파라미터의 사이즈가 2-프레임 모델보다 10배정도 높아 학습을 하는 것이 어려워지기 때문인 것으로 보입니다. 그러나 우리가 제안한 기술을 통해 multi-frame regression layer를 2-프레임 모델로 초기화 하면 2-프레임 모델보다 더 나은 성능을 보이며 이는 더 큰 temporal context 때문으로 생각됩니다.

temporal window W가 계속적으로 커지기 되면 제안된 초기화 기법을 사용해도 성능이 감소합니다. 이는 temporal window가 너무 커지게 되면 오브젝트의 움직임이 너무 복잡하게 되어 TPN이 근처의 프레임과의 visual correspondence를 복구하지 못하기 때문으로 보입니다. 이후의 실험에서는 우리는 5-프레임 TPN을 사용하여 20-프레임 tubelet을 생성합니다. 

우리가 제안한 모델과 RNN 베이스 라인을 비교하자면 tubelet regression layer를 1024 hidden의 RNN으로 대체했고 4 motion target을 예측합니다. 테이블 1에서 볼 수 있듯이 RNN 베이스라인의 성능은 우리보다 좋지 않습니다.

### 5.4. LSTM Training

tubelet proposal을 생성한 뒤에 제안된 CNN-LSTM 모델이 feature를 추출합니다. 이 feature의 차원은 각 타임마다 1024가 됩니다. 

LSTM은 1024개의 cell unit과 1024 hidden state를 가집니다 매번 iteration 마다 4개의 비디오에서의 128 tublet이 랜덤하게 선택되어 미니배치를 구성합니다. CNN-LSTM은 SGD를 통해 최적화 되며 momentum 0.9 로 20k iteration 동안 학습됩니다. 파라미터의 분산은 0.0002가 되도록 초기화 하고 learning rate는 0.1 2k iteration 마다 0.5씩 낮아지게 됩니다.

### 5.5. Results

<b>Baseline method</b> 가장 기본적인 베이스라인은 Fast RCNN static detector 이며 매 프레임 마다의 static proposal을 필요로 하며 temporal information을 사용하지 않습니다. 이 베이스라인은 우리가 사용한 RPN과 똑같은 static proposal을 사용하며 Fast R-CNN 모델을 인셉션을 백본으로 사용했습니다. 우리의 tubelet regression target의 효율을 검증하기 위해 gt의 location을 spatial anchor로 사용하여 tubelet proposal을 수행하였습니다.(?뭐라는지 모르겠음) 그 후 우리는 vanilla LSTM을 적용하고 이를 LocTubelet+LSTM 이란 이름으로 결과를 보고합니다. 우리의 tubelet proposal 방법은 MoiveTubelet 이며 우리는 KCF[10] 이라는 최신의 single-object trackin 방법과 비교합니다. CNN-LSTM classfication을 위해 vanilla LSTM과 encoder-decoder LSTM을 비교합니다. 

<b>Results on ImageNet VID dataset. </b>  VID데이터에 대한 결과는 테이블2,3 에 나와있습니다. 이미지넷에서의 detecion task의 규칙에 따라 우리는 validation set에 대한 결과를 보고 합니다. VID에 fine-tune된 Fast RCNN 베이스 라인은 0.63% 을 달성하였고 bset single model[14]의 성능과 비교하자면 VID 데이터셋에 0.615%를 얻어 1.5%의 성능향상이 있었습니디ㅏ.

베이스라인 static detector를 TPN에 바로 적용하고 temporal window 5의 결과를 갖는 map는 0.623% 을 얻었습니다. 기존의 최신의 tracker[10] 과 비교하자면 이는 오직 0.567% map을 얻었습니다. KCF tracker는 단일 오브젝트를 추적하는데 50fps가 걸리지면 이 베이스 라인 detector는 6초가 걸리게 됩니다.

vanilla LSTM에 tubelet proposal을 적용하면 0.67% map를 얻을 수 있고 이는 5.5%의 성능 향상을 가져왔고 static 베이스 라인에 비해 4.8% 향상입니다. 이는 LSTM은 appearance , temporal feature를 배울수 있어 classfication accuracy를 높일 수 있다는 것을 보여줍니다. 특히 'whale' 클래스에 대해서는 25% 의 큰 향상을 보였는데 고래는 물에서 떠올랐다가 다시 가라않습니다. detector는 이들을 판별하기 위해 전체 process를 관찰해야 합니다.

bounding box regression tubelet proposal과 비교하자면 우리의 tublet proposal 모델은 2.5% 의 향상을 보였고 temporal information을 더 잘 활용해 더 다양한 tubelet을 생성합니다. EM-LSTM 을 사용한 결과 0.68% 의 성능을 보였고 기본 LSTM보다 0.6% 의 성능 향상을 보였고 절반이상의 클래스에 대해 성능향상을 보였습니다. EM LSTM이 모든 클래스의 tubelet 베이스라인 결과보다 좋은 성능을 보였고 이는 detection 결과의 consistently 한 것을 향상시켰다는 의미입니다. VID에 대한 결과는 피규어5에 나와있으며 bodung box는 객체에 잘 붙어 있으며 긴 시간동안 여러개의 객체를 추적 할 수 있습니다.

<b>Localization on the YouTubeObjects dataset. </b> 추가적으로 YTO 데이터셋에 대해 평가를 진행했습니다. 각각의 테스트 비디오마다 우리는 tublet proposal을 생성하고 EM LSTM 을 적용하여 각 tublet을 판별합니다. 각 테스트 클래스마다 우리는 테스트 프레임에서 가장 maximum detection score를 가지는 tubelet box를 선택합니다.



