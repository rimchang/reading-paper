# How do CNNs Deal with Position Differences?

https://petewarden.com/2017/10/29/how-do-cnns-deal-with-position-differences/

이미지 인식을 위한 CNN을 공부하던 한 엔지니어가 나에게 재미있는 질문을 던졌다. 이미지안의 다른 위치에 있는 객체들을 어떻게 인식 할 수 있는가? 이러한 질문은 매우 긴 설명이 필요하므로 이러한 글을 적게 되었다.

<img src="https://petewarden.files.wordpress.com/2017/10/cnn-position-0.png?w=768" />

만약 이미지 안에서 해를 인식하려고 한다면 모델이 해가 이미지 안의 어디에 있는지를 어떻게 알게 하는 걸까요? 이는 매우 중요한 문제입니다. 이 문제를 사람들은 3가지 단계로 이해합니다.

- 만약 컴퓨터로 프로그래밍 하려는 것이 아니라면, 이는 매우 쉬운 문제입니다 단지 우리의 눈과 뇌는 이러한 다른 위치를 다루는데 아무런 문제가 없습니다.

- 만약 이러한 문제를 전통적인 프로그래밍 기법으로 다루려고 한다면 이들이 매우 어렵다는 것을 알기 때문에 가슴이 철렁 내려앉을 것입니다. 그리고 우리의 클라이언트에게 이들이 어렵다는 것을 설명하는 것은 매우 어렵습니다.

- 딥러닝에 익숙하다면, 우리의 네트워크가 그들의 stride에서 이러한 문제들을 다룰 것이고 미소를 짓게 될 것입니다.


내 친구는 딥러닝을 잘 아는 3번째 단계지만 CNN이 왜 position invariant 한지에 대한 설명이 거의 없다는 것을 알만큼 영리합니다. 저는 제 자신이 새로운 통찰력을 가지고 있다고 주장하지는 않습니다. 하지만 지난 몇년간 이미지 모델링 경험 통해 몇 가지 아이디어를 얻었고 내가 아는 것을 공유하기로 했습니다. 이는 매우 엔지니어링적인 직관을 통해 쓰여졌고 다른 논문들이 있다면 링크를 해주세요 더 나은 설명을 위해 참고하고 싶습니다.
 
 이 문제를 이해하기 위한 시작점은 네트워크가 본질적으로 position invariant 하지 않다는 것입니다. 이미지넷을 통해 학습시킨 네트워크를 핸드폰에서 실행했을때 우연히 발견했습니다. 이미지넷의 역사는 매우 재밌습니다. 원래는 구글 이미지 검색은 각 클래스의 이름을 검색함으로써 공공 웹사이트에서 후보 이미지를 검색 하기 위해 사용되었고 연구원들은 후보 이미지를 살펴보고 잘못된 검색결과를 삭제 했습니다. 내 친구인 Tom은 결과 데이터를 탐색하면서 재밌는 것을 찾아냈습니다. garbage truck 카테고리에 매우 많은 female이 있다는 것을 발견했습니다. 또한 이미지넷의 특성에 대해 더 자세히 이해하기 위해 이미지넷 사진을 수작업으로 라벨링 하는 카파시의 작업을 확인해 필요가 있다.
 
 우리의 목적은 트레이닝 데이터의 모든 이미지들은 일반 사람들이 찍은 것이고 이 트레이닝 데이터를 웹 검색에서 잘 랭크된 웹사이트에 게시하는 것입니다???.(잘 모르겠다) 이것의 의미는 random snapshot 보다 전문적이고, 피사체가 중앙에 가깝고, 수평에 위치하고, 많은 이미지를 모으는 것입니다. 대조적으로 classfier를 시험해 보기 위한 사람은 이상한 각도로 찍을 가능성이 높습니다. 누군가는 위에서 찍을 수도 있고 객체의 일부만을 찍을 수 있습니다. 이 것은 이미지넷에서 학습된 모델이 낮은 성능을 보이는 것을 보일 것입니다. 핸드폰에서 실행될때 이미지넷의 트레이닝 셋과 사용자의 이미지가 매우 다르기 때문에 논문에서 게시된 accuracy보다 훨씬 낮은 성능을 보일 것입니다. 안드로이드에서 Tensorflow classify 어플을 사용한다면 스스로 경험해 볼 수 있습니다. 사용자에게 어떻게 하라는 지시사항이 주어지기 때문에 휴대폰에서는 그렇게 문제가 되지 않습니다. 하지만 이러한 것은 로봇과 같은 장치에서 더 심각한 문제입니다. 이들의 카메라 배치를 알 수 없기 때문에 이미지넷에서 트레인된 모델은 심각한 문제를 겪게 됩니다. 나는 이러한 어플리케이션의 개발자에게 비슷한 장치의 환경에서 트레이닝 셋을 만들기를 추천합니다. 종종 완전 다른 fisheye 렌즈같은 것도 사용되기 때문입니다.
 
 (정리하자면,, 이미지넷은 잘 정리된,, 수평과, 객체가 중앙에 위치하도록 잘 정리된 데이터 셋이다. 핸드폰과 같은 다른 환경에서는 잘 작동하지 않게 된다. 카메라의 위치에 따라 다르게 작동한다)
 
 환경의 차이뿐만 아니라 이미지넷 데이터셋 안에도 매우 많은 position variance가 존재합니다. 그렇다면 네트워크는 어떻게 이런 것을 처리할까요 이러한 비밀의 일부는 인풋 이미지에 인공적인 offset(변화? 하여튼 data argument를 뜻하는듯) 을 추가하기 때문에 네트워크는 이러한 다름에 대해 배워야 합니다.
 
 <img src="https://petewarden.files.wordpress.com/2017/10/cnn-position-1.png?w=768" />
 
 네트워크의 인풋으로 이미지를 넣기 전에 랜덤하게 crop 할 수 있습니다. 인풋 이미지가 200x200 과 같이 정사각형의 크기로 제한되어져 있기 때문에 이러한 랜덤 crop은 각 이미지의 객체의 scale, postion을 randomize 하는 효과를 가집니다. 네트워크는 예측에 대해 처벌받거나 보상받기 때문에 이러한 차이에도 불구하고 올바르게 예측하는 성능을 가지게 됩니다. 이는 네트워크가 postion의 변화에 왜 대처하는지를 설명하지만 어떻게 대처하는지는 설명하지 못합니다.
 
 이를 조사하기 위해서, 약간의 유추와 folklore(민간적으로 내려오는 속담,, 같은것 명확히 연구되지는 않았지만 받아들여질 만하다?) 을 해야만 합니다. 저는 제가 제시하는 것에대해 설명하는 논문을 찾을 수 없었지만 다른 실무자와의 논의와 토론을 통해 이는 실무적인 이론으로 받아질만한 것으로 보입니다.
  
Alexnet 이후로 CNN은 최종의 예측을 위해 연속적인 레이어로 구성되어져 있습니다. 초기 레이어들이 edge와 같은 매우 기본적인 패턴을 찾고 그 이후의 레이어들은 더 깊어질 수록 높은 레벨의 개념을 찾습니다. 일반적인 레이어의 첫번째 층을 봄으로써 쉽게 확인 할 수 있습니다.
 
  
 <img src="https://devblogs.nvidia.com/parallelforall/wp-content/uploads/2014/10/caffenet_learned_filters.png" />
 
 위의 그림이 보여주는 것은 각 필터가 찾는 패턴을 보여줍니다. 몇몇은 다른 방향의 edge이며 색상, 모서리의 패턴을 찾습니다. 불행히도 첫번째 층 이후의 레이어들은 간단하게 시각화 할 수 없습니다. 그 이후의 레이어를 시각화 하고 싶다면 Jason Yosinski이나 다른 연구자의 연구를 찾아보세요
 
 
  <img src="https://petewarden.files.wordpress.com/2017/10/cnn-position-2.png" />
  
위의 그림으로 봉주고 싶은 것은 첫번째 레이어는 이미지에서 매우 간단한 패턴, 수평 edge, 모서리, 특정 색의 patch 들을 찾는 다는 것입니다. 이는 CaffeNet 그림(이전 그림의 필터시각화 그림)에서 표현된 것과 매우 비슷합니다.이들의 결과는 각 패턴이 일치하는 위치를 강조하는 heat map 으로 출력됩니다.

 이해하기 어려운 점은 두번째 층에서 일어나는 일입니다. 첫번째 레이어의 간단한 필터의 heatmap은 activation layer의 분리된 채널에 들어가므로 두번째 레이어의 인풋은 일반적으로 몇백개 입니다.(두번째 레이어의 인풋은 첫번째 레어이어에서의 필터 갯수만큼의 채널을 가진다는 뜻) 이는 일반적인 3,4 채널을 가지는 이미지와 다릅니다. 두번째 레이어에서 찾고 있는 것은 첫번째 레이어의 heatmap이 결합된 더 복잡한 패턴입니다. 위의 그림에서 태양의 한 petal을 인식하려고 합니다. 우리는 모서리에 이러한 petal이 있다는 것을 압니다. 근처는 수직선이 위치하고 이들은 노랑색으로 채워져 있다는 것을 알고 있습니다. 각각의 개별적인 특징은 하나의 채널에 의해 representation 됩니다. 두번째 레이어의 "petal facing left" 를 찾기위한 필터는 이러한 세가지 패턴이 모두 나타나는 패턴을 찾습니다. 이런 세가지 패턴중 한두개만 나타나는 이미지는 아무런 출력이 없지만 세가지 패턴이 모두 나타나면 높은 activation을 가지게 됩니다.
 
 첫번째 레이어와 비슷하게 두번째 레이어에도 매우 많은 필터가 있습니다. 그리고 각 필터는 "petal facing up", "petal facing right", 등과 같은 더 높은 레벨의 개념을 나타낼 수 있습니다. 이들은 하나의 필터가 어떠한 개념을 나타내는지를 시각화 하기는 어렵습니다.
 
 네트워크가 더 깊어질수록 이러한 개념은 더 높은 수준으로 가게 됩니다. 예를 들어 여기서 세번째, 네번째 렝어는 꽃잎으로 둘러쌓인 노란색 원을 활성화 할 수도 있습니다. 이러한 representation을 태양이라고 분류하는 classfier를 만드는 것은 매우 쉬운 일입니다. 물론 실제의 classfier는 우리가 위에서 언급한 것처럼 깔끔한 representation을 하지는 못합니다. 이들은 인간과 친숙한 개념을 배우는 것이 아닌 그들 스스로 문제를 해결하는 방법을 배우기 때문입니다. 하지만 기본적인 아이디어는 같습니다.
 
 이는 네트워크가 position의 차이를 어떻게 해결하는 지를 설명하지는 않습니다. 이를 이해하려면 이미지 인식을 위한 CNN의 공통된 디자인에 대해 알아야 합니다. 네트워크가 깊어질 수록 채널의 수는 증가하지만 이미지의 크기는 줄어들게 됩니다. 이러한 줄어듬은 Pooling 레이어를 통해 이뤄집니다. 전통적으로는 average pooling을 썻지만 요즘엔 max pooling을 사용합니다. 어쨋든 효과는 비슷합니다.
 
  
  <img src="https://petewarden.files.wordpress.com/2017/10/max-pooling.png" />
  
  여기서 우리는 이미지를 반으로 줄이는 것을 볼 수 있습니다. 각 출력 픽셀에 대해 2x2 patch를 살펴보고 max값을 선택합니다. 그래서 max pooling이라고 불립니다. average pooling은 max대신 mean을 사용합니다.
  
  이러한 종류의 pooling은 네트워크를 거치면서 반복적으로 적용됩니다. 이것의 의미는 네트워크의 끝에서는 300x300 의 이미지가 13x13으로 줄어들 수 있다는 것입니다. 또한 이러한 주러듬은 가능한 position의 변화가 줄어듬을 의미합니다. 위의 예제에서는 오직 13개의 열과 행에만 태양이 나타날 수 있습니다. (원래는 300x300의 위치에 태양이 있는지를 알아야 했지만... 풀링을 거치면서 13x13의 위치에 태양이 있는지만 알면된다는 뜻인듯.) 어느 정도의 position의 차이는 숨겨지게 되는데 이는 activation이 맥스풀링을 통해 하나의 cell로 합쳐지기 때문입니다. 이것은 position의 차이를 마지막 분류기가 더 쉽게 다루게 해줍니다. 마지막의 분류기는 원래의 이미지보다 훨씬 간단한 representation을 다루게 됩니다.
   
 이것은 이미지 classfier가 position의 변화를 어떻게 다루는지에 대한 설명이지만 오디오와 같은 문제에서는 어떨까요 저는 최근에 다른 pooling을 사용하는"dilated", "atrous" convolution에 대해 관심을 가지고 있습니다. 맥스풀링과 마찬가지로 이는 작은 이미지를 생성하지만 convolution 자체에서의 맥락에서 이를 수행합니다. 인접한 픽셀에 대해 풀링을 진행하는 것 대신 이들은 매우 크게 될수도 있는 stride의 픽셀들에 대해 진행합니다. 이것은 non-local information을 다룰 수 있도록 하고 (딥마인드의 Wavenet 논문에서 처럼) rnn을 쓰지 않고 cnn을 사용하여 시간적 정보를 다룰 수 있는 능력을 줍니다.
 
 RNN을 빠르게 하는 것은 매우 힘들기 때문에 이것은 매우 놀랍습니다. 한개의 batch size를 다룰 때 (실시간 어플리케이션에서 보통 그렇다). 대부분의 연산은 fc layer와 같이 매트릭스와 vector 곱입니다. 모든 weight들이 한번만 계산되므로 일반적으로 convolution의 경우와 같이 계산시간이 제한이 아니라 메모리가 제한점이 됩니다. 나는 이들이 다른 분야에서 더 잘 보이게 될것이라고 생각합니다.
 
 정리 CNN이 postion invariant 한 이유
 
 1, data argument : 데이터 argu를 통해 다양한 위치에서의 객체를 네트워크가 배울 수 있게 한다.  
 2, pooling : pooling을 통해 300x300에서 13x13으로 줄어들었다고 생각해보자 마지막의 분류기가 배울 위치가 300x300에서 13x13으로 줄어듬.. 
 
 